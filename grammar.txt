steps
    preparation :
        dataselection :
            import <file> for test
            import <file> for train
            import <file> // we do the split ourselves

        preprocessing :
            vars :
                seed <number>
                training_size <number>
            methods :
                <method> <args> ->
                - handle_null remove
                - outliers remove
                - drop [unused columns]

        transformation :
                random_forest : randomforest
                    <method> <args> ->
                    - normalize 255
                    - pca  nb_comp: number
                    - std_scaler
                    - minmax

    models :
        datamining :
            <training name> :
                <algo> <algoname>
                <transformation name>

                <train method> :
                <params>
                    a : 3

            <training name>
                <algo> <algoname>

                <params>

                    a = 3

    testing :
        validation :
            <method> :
                <params>
                    cv : 3
                    scoring

        comparaison
            criteria : scores







imports <folder path>

# chaine de traitement
## Selection des donnees
    dans cette etape on choisit les donn√©es sur lequelle on va travailler
## Pre-traitement
### clean les donnees
- ajoutant des valeurs manquant
- Gestion des extremes
- suppression des duplica
- ...
## Transformation
l objectif c'est de transformer les donnees brutes en donnees qui peuvent etre utiliser apres dans l etape de training
###
- Agregation
- DIscretisation
- uniformisation
- ...
## Data mining <Training>
Dans cette etape on entraine notre model en executant des algo comme
- Regression
- Classification
- Clustering
- ....
## validation <Testing>
- Evaluation sur l ensemble de validation
- mesure de l'erreur

# repetitive pattern
pipe_xx
rand_list_xx
gs_xx
score_xx

# exmaple

## Data selection
import pandas as pd
sample_submission = pd.read_csv("../input/digit-recognizer/sample_submission.csv")
test = pd.read_csv("../input/digit-recognizer/test.csv")
train = pd.read_csv("../input/digit-recognizer/train.csv")
X_train = train.loc[:,"pixel0":"pixel783"]
Y_train = train.loc[:, "label"]
X_test = test

## Transformation
RANDOM_STATE = 42
pca = PCA(n_components=0.95)
kfold = StratifiedKFold(n_splits=2, shuffle = True)
kfold





## training
#KNN
### model = pluged models
pipe_knn = Pipeline([('StdScaler', StandardScaler()),
('minmax', MinMaxScaler()),
('pca', pca),
('clf_knn', KNeighborsClassifier())])


### hyper param possible values
rand_list_KNN = {'clf_knn__n_neighbors': sp_randint(1, 11),
'clf_knn__algorithm': ['auto']}

### training
gs_KNN = RandomizedSearchCV(estimator = pipe_knn,
param_distributions = rand_list_KNN,
cv = kfold,
verbose = 2,
n_jobs = -1,
n_iter = 5)


## validation
scoring = {'acc': 'accuracy'}
scores_knn = cross_validate(gs_KNN, X_train, Y_train, cv = 5, scoring = scoring)

print('Cross Validation Scores:')
print('Accuracy: %.4f +/- %.4f' % (np.mean(scores_knn['test_acc']), np.std(scores_knn['test_acc'])))